# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This is a computational text analysis project for distant reading of classic Russian literature. The repository contains two major works and a complete analysis pipeline:

- `Chernyshevsky_What_Is_To_Be_Done_UTF8.txt` - Nikolai Chernyshevsky's "What Is To Be Done?" (1863), ~195K words
- `pg600.txt` - Fyodor Dostoyevsky's "Notes from the Underground" (Project Gutenberg #600), ~45K words

## Common Commands

### Running the Analysis Pipeline

```bash
# 1. Preprocess texts (remove PG headers, normalize formatting)
python3 scripts/preprocess.py

# 2. Run comprehensive analysis (generates results/analysis.json)
python3 scripts/analyze.py

# 3. View interactive visualization
# Open results/index.html in any web browser
```

### File Structure

```
scripts/
├── preprocess.py    # Text cleaning and normalization
└── analyze.py       # Comprehensive distant reading analysis

data/processed/      # Cleaned texts (generated by preprocess.py)
results/
├── analysis.json    # Complete analysis results in JSON format
└── index.html       # Self-contained interactive visualization
```

## Analysis Architecture

### Analysis Pipeline (analyze.py)

The analysis script performs a comprehensive distant reading workflow:

1. **Text Loading & Tokenization** - Loads preprocessed texts and extracts sentences/words
2. **Word Frequency Analysis** - Generates top words (all + content words), bigrams, trigrams
3. **Vocabulary Richness** - Calculates type-token ratio, lexical diversity, hapax legomena
4. **Statistical Analysis** - Sentence/word length distributions
5. **Part-of-Speech Distribution** - Pattern-based POS tagging (simplified approach)
6. **Readability Scoring** - Flesch Reading Ease and Flesch-Kincaid Grade Level
7. **Sentiment Analysis** - Positive/negative word lexicon approach
8. **Named Entity Extraction** - Capitalized word pattern matching
9. **Comparative Analysis** - Side-by-side metrics and distinctive vocabulary identification

**Output:** Single JSON file (`results/analysis.json`) containing all analysis results for both texts plus comparison data.

### Interactive Visualization (index.html)

Self-contained single-page application with embedded analysis data (no external dependencies except CDN libraries):

- **Technology:** Vanilla JavaScript + Chart.js + WordCloud2.js
- **Data Embedding:** JSON analysis results are embedded directly in the HTML (no fetch required)
- **Features:**
  - Tabbed navigation (Dostoyevsky / Chernyshevsky / Comparison)
  - Word clouds for each text
  - Sentiment and readability displays
  - Part-of-speech distribution charts
  - Overlay comparison charts
  - Statistical comparison tables

**Important:** The HTML file works when opened directly in a browser (file:// protocol) because data is embedded, not fetched.

## Analysis Methods

All analyses use custom Python implementations (no external NLP libraries required):

- **Sentiment:** Simple positive/negative word lexicons (~80 words total)
- **POS Tagging:** Pattern-based estimation using common word endings
- **Readability:** Syllable counting + Flesch formulas
- **NER:** Capitalized word pattern matching with frequency filtering

**Note:** For production-grade analysis, consider integrating NLTK, spaCy, or TextBlob. Current implementation prioritizes zero external dependencies.

## Modifying Analysis

### Adding New Metrics

To add new analysis metrics:

1. Add analysis function to `scripts/analyze.py` (follow existing pattern)
2. Call function in `analyze_text()` and add results to analysis dictionary
3. Update `results/index.html` to display new metrics in relevant tab sections
4. Re-run: `python3 scripts/analyze.py`

### Adding New Texts

1. Place new text file in repository root
2. Update `scripts/preprocess.py` `main()` function to include new text
3. Update `scripts/analyze.py` `main()` to analyze new text
4. Update `results/index.html` to add new tab/sections for the text
5. Run preprocessing and analysis

## Data Flow

```
Original Texts (root/)
  → preprocess.py
  → Cleaned Texts (data/processed/)
  → analyze.py
  → Analysis Results (results/analysis.json)
  → Embedded in index.html for visualization
```

## Text Encoding

- Both source texts are UTF-8 encoded
- Preprocessed texts maintain UTF-8 encoding
- HTML uses UTF-8 meta charset for proper display of special characters
